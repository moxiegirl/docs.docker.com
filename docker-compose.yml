data:
    image: busybox
    command: "true"
    volumes:
        - /docs
        - /public

fetch:
    build: .
    command: |
        /bin/bash -ec " \
        python fetch_content.py /docs all-projects.yml
        ./touch-up.sh /docs
        "
    volumes_from:
        - data
    environment:
        - GITHUB_USERNAME
        - GITHUB_TOKEN

build:
    build: .
    command: /bin/bash -ec "hugo -d /public/$DOCS_VERSION --config=config.toml"
    working_dir: /docs
    volumes_from:
        - data
    environment:
        - DOCS_VERSION

upload:
    build: .
    working_dir: /public
    command: |
        /bin/bash -c ' \
        if [ -n "$CLEAN" ] ; then
          aws s3 rm --recursive s3://$AWS_S3_BUCKET/$CLEAN
        fi
        aws s3 sync --acl=public-read . s3://$AWS_S3_BUCKET
        '
    volumes_from:
        - data
    environment:
        - CLEAN
        - AWS_ACCESS_KEY_ID
        - AWS_SECRET_ACCESS_KEY
        - AWS_S3_BUCKET

cleanup:
  build: .
  command: |
    /bin/bash -c ' \
    if [[ "$AWS_S3_BUCKET" =~ "/" ]] ; then
      BUCKET_PATH=$( echo "$AWS_S3_BUCKET" | sed "s/[^\/]*\///" )
      BUCKET_PATH+="/"
      AWS_S3_BUCKET=$( echo "$AWS_S3_BUCKET" | sed "s/\/.*//")
    else
      BUCKET_PATH=
    fi

    [ -z "$RM_OLDER_THAN" ] && exit 1
    CUTOFF_UNIX_TS=$( date --date "$RM_OLDER_THAN" '+%s' )
    aws s3 ls --recursive s3://$AWS_S3_BUCKET/$BUCKET_PATH | while read -a LINE ; do
      DATE="${LINE[0]}"
      TIME="${LINE[1]}"
      SIZE="${LINE[2]}"
      NAME="${LINE[*]:3}"

      VERSION_REGEX="^${BUCKET_PATH}v[0-9]+\.[0-9]+/"
      UNIX_TS=$( date --date "$DATE $TIME" "+%s" )

      if [[ "$NAME" =~ $VERSION_REGEX ]] || [[ "$CUTOFF_UNIX_TS" -le "$UNIX_TS" ]] ; then
        echo "Keeping $NAME"
        continue
      fi

      echo "Creating redirect for $NAME"
      aws s3 cp "s3://$AWS_S3_BUCKET/$NAME" "s3://$AWS_S3_BUCKET/$NAME" --website-redirect="/${BUCKET_PATH}index.html" --acl=public-read > /dev/null
    done
    '
  environment:
    - RM_OLDER_THAN
    - AWS_ACCESS_KEY_ID
    - AWS_SECRET_ACCESS_KEY
    - AWS_S3_BUCKET

serve:
    build: .
    working_dir: /docs
    command: /bin/bash -c "hugo server -d /public --port=8000 --baseUrl=$HUGO_BASE_URL --bind=$HUGO_BIND_IP --config=config.toml"
    environment:
        - HUGO_BASE_URL
        - HUGO_BIND_IP
    ports:
        - "8000:8000"
    volumes_from:
        - data

test:
    build: .
    working_dir: /docs
    command: |
        /bin/bash -c " \
        URL=http://$HUGO_BASE_URL:8000
        hugo server -d /public --port=8000 --baseUrl=$HUGO_BASE_URL --bind=$HUGO_BIND_IP --config=config.toml &
        until curl --fail --silent $URL ; do \
          sleep 1 ; \
        done
        exec python /src/docvalidate.py $URL
        "
    environment:
        - HUGO_BASE_URL
        - HUGO_BIND_IP
    ports:
        - "8000:8000"
    volumes_from:
        - data
